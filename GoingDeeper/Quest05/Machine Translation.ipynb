{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb7ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8f6b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b05e3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "faea7c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12ì‹œ ë•¡!</td>\n",
       "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
       "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL ì‹¬í•˜ë„¤</td>\n",
       "      <td>ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
       "      <td>í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
       "      <td>í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>í‘ê¸°ì‚¬ í•´ì£¼ëŠ” ì§ë‚¨.</td>\n",
       "      <td>ì„¤ë œê² ì–´ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>í˜ë“  ì—°ì•  ì¢‹ì€ ì—°ì• ë¼ëŠ”ê²Œ ë¬´ìŠ¨ ì°¨ì´ì¼ê¹Œ?</td>\n",
       "      <td>ì˜ í—¤ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ì´ ì—¬ë¶€ì¸ ê±° ê°™ì•„ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>í˜ë“¤ì–´ì„œ ê²°í˜¼í• ê¹Œë´</td>\n",
       "      <td>ë„í”¼ì„± ê²°í˜¼ì€ í•˜ì§€ ì•Šê¸¸ ë°”ë¼ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12ì‹œ ë•¡!                í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0\n",
       "1                  1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´                 ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0\n",
       "2                 3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤               ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "3              3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤               ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "4                      PPL ì‹¬í•˜ë„¤                ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0\n",
       "...                        ...                       ...    ...\n",
       "11818           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.        í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !      2\n",
       "11819           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.             í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.      2\n",
       "11820              í‘ê¸°ì‚¬ í•´ì£¼ëŠ” ì§ë‚¨.                    ì„¤ë œê² ì–´ìš”.      2\n",
       "11821  í˜ë“  ì—°ì•  ì¢‹ì€ ì—°ì• ë¼ëŠ”ê²Œ ë¬´ìŠ¨ ì°¨ì´ì¼ê¹Œ?  ì˜ í—¤ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ì´ ì—¬ë¶€ì¸ ê±° ê°™ì•„ìš”.      2\n",
       "11822               í˜ë“¤ì–´ì„œ ê²°í˜¼í• ê¹Œë´        ë„í”¼ì„± ê²°í˜¼ì€ í•˜ì§€ ì•Šê¸¸ ë°”ë¼ìš”.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatbotData = pd.read_csv('~/aiffel/transformer_chatbot/data/ChatbotData.csv')\n",
    "ChatbotData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12e8e5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatbotData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1f7d7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n"
     ]
    }
   ],
   "source": [
    "ChatbotData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fa386c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(ChatbotData, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a22cde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨ ë°ì´í„° ìˆ˜: 10640\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜: 1183\n"
     ]
    }
   ],
   "source": [
    "# ê° ë°ì´í„°ì—ì„œ ì§ˆë¬¸(Q)ê³¼ ë‹µë³€(A) ë¶„ë¦¬\n",
    "train_que = train_data['Q'].tolist()\n",
    "train_ans = train_data['A'].tolist()\n",
    "\n",
    "test_que = test_data['Q'].tolist()\n",
    "test_ans = test_data['A'].tolist()\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ìˆ˜: {len(train_que)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜: {len(test_que)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5b97ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Questions: ['12ì‹œ ë•¡!', '1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´', '3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤']\n",
      "Sample Answers: ['í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.', 'ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.', 'ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .']\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"Sample Questions:\", questions[:3])\n",
    "print(\"Sample Answers:\", answers[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1dde506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # ì˜ë¬¸ì, í•œê¸€, ìˆ«ì, ê³µë°±, ì£¼ìš” íŠ¹ìˆ˜ë¬¸ì(.,!?~)ë§Œ ë‚¨ê¸°ê³  ì œê±°\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9ê°€-í£\\s.,!?~]', '', sentence)\n",
    "    \n",
    "    # ë¬¸ìì—´ ì–‘ìª½ ê³µë°± ì œê±°\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1faeded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello! ì•ˆë…•í•˜ì„¸ìš”?  ì—°ìŠµì…ë‹ˆë‹¤!!\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "test_sentence = \"Hello! ì•ˆë…•í•˜ì„¸ìš”? ğŸ˜Š ì—°ìŠµì…ë‹ˆë‹¤!!\"\n",
    "print(preprocess_sentence(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a064aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab ê°ì²´ ìƒì„±\n",
    "mecab = Mecab()\n",
    "\n",
    "def build_corpus(source_data, target_data, tokenize_fn, max_token_length=50):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  í† í°í™”í•˜ì—¬ ë§ë­‰ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        source_data (list of str): ì†ŒìŠ¤ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ì§ˆë¬¸)\n",
    "        target_data (list of str): íƒ€ê²Ÿ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ë‹µë³€)\n",
    "        tokenize_fn (function): ì‚¬ìš©í•  í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ (ex: mecab.morphs)\n",
    "        max_token_length (int): ìµœëŒ€ í† í° ê¸¸ì´ (ê¸°ë³¸ê°’: 50)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ì†ŒìŠ¤ ë§ë­‰ì¹˜, íƒ€ê²Ÿ ë§ë­‰ì¹˜)\n",
    "    \"\"\"\n",
    "    def preprocess_and_tokenize(data):\n",
    "        processed = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì§‘í•©\n",
    "        tokenized_data = []  # ìµœì¢… í† í°í™”ëœ ë¬¸ì¥ ì €ì¥\n",
    "        \n",
    "        for sentence in data:\n",
    "            # 1. ë¬¸ì¥ ì •ì œ\n",
    "            cleaned_sentence = preprocess_sentence(sentence)\n",
    "\n",
    "            # 2. ë¬¸ì¥ í† í°í™”\n",
    "            tokens = tokenize_fn(cleaned_sentence)\n",
    "\n",
    "            # 3. ìµœëŒ€ í† í° ê¸¸ì´ ì´ˆê³¼ ì‹œ ì œì™¸\n",
    "            if len(tokens) <= max_token_length:\n",
    "                # 4. ì¤‘ë³µëœ ë¬¸ì¥ì€ ì¶”ê°€í•˜ì§€ ì•ŠìŒ\n",
    "                joined_tokens = ' '.join(tokens)  # í† í°ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ ì¤‘ë³µ ê²€ì‚¬\n",
    "                if joined_tokens not in processed:\n",
    "                    processed.add(joined_tokens)\n",
    "                    tokenized_data.append(tokens)\n",
    "\n",
    "        return tokenized_data\n",
    "\n",
    "    # ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë°ì´í„° ê°ê°ì— ëŒ€í•´ ì²˜ë¦¬\n",
    "    que_corpus = preprocess_and_tokenize(source_data)\n",
    "    ans_corpus = preprocess_and_tokenize(target_data)\n",
    "\n",
    "    return que_corpus, ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b6bf455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions Corpus Sample: [['12', 'ì‹œ', 'ë•¡', '!'], ['1', 'ì§€ë§', 'í•™êµ', 'ë–¨ì–´ì¡Œ', 'ì–´'], ['3', 'ë°•', '4', 'ì¼', 'ë†€', 'ëŸ¬', 'ê°€', 'ê³ ', 'ì‹¶', 'ë‹¤']]\n",
      "Answers Corpus Sample: [['í•˜ë£¨', 'ê°€', 'ë˜', 'ê°€', 'ë„¤ìš”', '.'], ['ìœ„ë¡œ', 'í•´', 'ë“œë¦½ë‹ˆë‹¤', '.'], ['ì—¬í–‰', 'ì€', 'ì–¸ì œë‚˜', 'ì¢‹', 'ì£ ', '.']]\n"
     ]
    }
   ],
   "source": [
    "# build_corpus í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ë­‰ì¹˜ ìƒì„±\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers, mecab.morphs)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"Questions Corpus Sample:\", que_corpus[:3])\n",
    "print(\"Answers Corpus Sample:\", ans_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42b46410",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_195/3680712994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word2Vec ëª¨ë¸ ë¡œë“œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/aiffel/transformer_chatbot/data/ko.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlexical_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1545\u001b[0m         \"\"\"\n\u001b[1;32m   1546\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Word2Vec ëª¨ë¸ ë¡œë“œ\n",
    "word2vec = KeyedVectors.load_word2vec_format('~/aiffel/transformer_chatbot/data/ko.bin', binary=True)\n",
    "\n",
    "def lexical_sub(sentence, wv):\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ì—ì„œ ì„ì˜ì˜ ë‹¨ì–´ë¥¼ ìœ ì‚¬í•œ ë‹¨ì–´ë¡œ êµì²´í•˜ëŠ” í•¨ìˆ˜.\n",
    "    Args:\n",
    "        sentence (list of str): í† í°í™”ëœ ì…ë ¥ ë¬¸ì¥\n",
    "        wv (gensim.models.KeyedVectors): Word2Vec ì„ë² ë”© ëª¨ë¸\n",
    "    Returns:\n",
    "        list of str: ìœ ì‚¬ ë‹¨ì–´ê°€ êµì²´ëœ ìƒˆë¡œìš´ ë¬¸ì¥ (í˜¹ì€ ì›ë³¸ ë¬¸ì¥)\n",
    "    \"\"\"\n",
    "    # Word2Vecì— ìˆëŠ” ë‹¨ì–´ë§Œ ì„ íƒ\n",
    "    valid_tokens = [tok for tok in sentence if tok in wv]\n",
    "\n",
    "    # ìœ íš¨í•œ ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë¬¸ì¥ ë°˜í™˜\n",
    "    if not valid_tokens:\n",
    "        return sentence\n",
    "\n",
    "    # ëœë¤ìœ¼ë¡œ êµì²´í•  ë‹¨ì–´ ì„ íƒ\n",
    "    selected_tok = random.choice(valid_tokens)\n",
    "\n",
    "    # ìœ ì‚¬í•œ ë‹¨ì–´ë¡œ êµì²´í•œ ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„±\n",
    "    new_sentence = [\n",
    "        wv.most_similar(tok, topn=1)[0][0] if tok == selected_tok else tok for tok in sentence\n",
    "    ]\n",
    "\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fa27e",
   "metadata": {},
   "source": [
    "### utf-8ì˜ ë¬¸ì œ\n",
    "- ko.binì„ ë¶ˆëŸ¬ì˜¬ ë•Œë§ˆë‹¤ utf-8 ì¸ì½”ë”© ì—ëŸ¬ê°€ ê³„ì† ë°œìƒ\n",
    "- ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê³ ì³ë³¼ë ¤ í–ˆì§€ë§Œ ì‹¤íŒ¨\n",
    "- ë²„ì „ì„ ë‹¤ìš´ê·¸ë ˆì´ë“œí•˜ë‹ˆ í•´ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b777f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '~/aiffel/transformer_chatbot/data/ko.bin'\n",
    "\n",
    "word2vec = Word2Vec.load(model_path).wv \n",
    "\n",
    "# Lexical Substitution í•¨ìˆ˜ ì •ì˜\n",
    "def lexical_sub(sentence, wv):\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ì—ì„œ ì„ì˜ì˜ ë‹¨ì–´ë¥¼ ìœ ì‚¬í•œ ë‹¨ì–´ë¡œ êµì²´í•˜ëŠ” í•¨ìˆ˜.\n",
    "    Args:\n",
    "        sentence (list of str): í† í°í™”ëœ ì…ë ¥ ë¬¸ì¥\n",
    "        wv (gensim.models.KeyedVectors): Word2Vec ì„ë² ë”© ëª¨ë¸\n",
    "    Returns:\n",
    "        list of str: ìœ ì‚¬ ë‹¨ì–´ê°€ êµì²´ëœ ìƒˆë¡œìš´ ë¬¸ì¥ (í˜¹ì€ ì›ë³¸ ë¬¸ì¥)\n",
    "    \"\"\"\n",
    "    # Word2Vecì— ìˆëŠ” ë‹¨ì–´ë§Œ ì„ íƒ\n",
    "    valid_tokens = [tok for tok in sentence if tok in wv]\n",
    "\n",
    "    # ìœ íš¨í•œ ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë¬¸ì¥ ë°˜í™˜\n",
    "    if not valid_tokens:\n",
    "        return sentence\n",
    "\n",
    "    # ëœë¤ìœ¼ë¡œ êµì²´í•  ë‹¨ì–´ ì„ íƒ\n",
    "    selected_tok = random.choice(valid_tokens)\n",
    "\n",
    "    # ìœ ì‚¬í•œ ë‹¨ì–´ë¡œ êµì²´í•œ ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„±\n",
    "    new_sentence = [\n",
    "        wv.most_similar(tok, topn=1)[0][0] if tok == selected_tok else tok for tok in sentence\n",
    "    ]\n",
    "\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c583952c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c7cad665054ac28f4f4a920fed8167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting Questions:   0%|          | 0/11643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb99a01af67946a195a307960455be6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting Answers:   0%|          | 0/7728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ìƒì„±ëœ ë°ì´í„° ìˆ˜: 34929\n",
      "ì²« ë²ˆì§¸ ë°ì´í„° ìŒ:\n",
      "Q: 12 ì‹œ ë— !\n",
      "A: í•˜ë£¨ ê°€ ë˜ ê°€ ë„¤ìš” .\n"
     ]
    }
   ],
   "source": [
    "# ìƒˆë¡œìš´ ë§ë­‰ì¹˜ ì´ˆê¸°í™”\n",
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "# 1. ì§ˆë¬¸(Questions) ë§ë­‰ì¹˜ì— ëŒ€í•´ Augmentation ìˆ˜í–‰\n",
    "for tokens in tqdm(que_corpus, desc=\"Augmenting Questions\"):\n",
    "    new_tokens = lexical_sub(tokens, word2vec)\n",
    "    new_que_corpus.extend([new_tokens, tokens])  # ì¦ê°•ëœ ë¬¸ì¥ê³¼ ì›ë³¸ ë¬¸ì¥ ì¶”ê°€\n",
    "\n",
    "# 2. ë‹µë³€(Answers) ë§ë­‰ì¹˜ì— ëŒ€í•´ Augmentation ìˆ˜í–‰\n",
    "for tokens in tqdm(ans_corpus, desc=\"Augmenting Answers\"):\n",
    "    new_tokens = lexical_sub(tokens, word2vec)\n",
    "    new_ans_corpus.extend([new_tokens, tokens])  # ì¦ê°•ëœ ë¬¸ì¥ê³¼ ì›ë³¸ ë¬¸ì¥ ì¶”ê°€\n",
    "\n",
    "# ë³‘ë ¬ ë°ì´í„° ìƒì„±: (Aug Q, Original A), (Original Q, Aug A), (Original Q, Original A)\n",
    "augmented_data = []\n",
    "\n",
    "# ìµœëŒ€ ê¸¸ì´ë¡œ ë§ì¶”ê¸° ìœ„í•´ í•œìª½ ë°ì´í„°ê°€ ë¶€ì¡±í•  ê²½ìš° ë°˜ë³µ ì‚¬ìš©\n",
    "max_pairs = max(len(que_corpus), len(ans_corpus))\n",
    "\n",
    "# 3ë°° ë°ì´í„° ìƒì„±\n",
    "for i in range(max_pairs):\n",
    "    # iê°€ ë²”ìœ„ë¥¼ ì´ˆê³¼í•  ê²½ìš° ë¬´ì‘ìœ„ ë°ì´í„° ì‚¬ìš©\n",
    "    q_idx = i % len(que_corpus)\n",
    "    a_idx = i % len(ans_corpus)\n",
    "\n",
    "    # (Augmented Q, Original A)\n",
    "    augmented_data.append((new_que_corpus[2 * q_idx], ans_corpus[a_idx]))\n",
    "\n",
    "    # (Original Q, Augmented A)\n",
    "    augmented_data.append((que_corpus[q_idx], new_ans_corpus[2 * a_idx]))\n",
    "\n",
    "    # (Original Q, Original A)\n",
    "    augmented_data.append((que_corpus[q_idx], ans_corpus[a_idx]))\n",
    "\n",
    "# ë°ì´í„° ê°œìˆ˜ í™•ì¸ ë° ì˜ˆì‹œ ì¶œë ¥\n",
    "print(f\"ì´ ìƒì„±ëœ ë°ì´í„° ìˆ˜: {len(augmented_data)}\")\n",
    "\n",
    "print(\"ì²« ë²ˆì§¸ ë°ì´í„° ìŒ:\")\n",
    "print(\"Q:\", ' '.join(augmented_data[0][0]))\n",
    "print(\"A:\", ' '.join(augmented_data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ee71e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ íƒ€ê²Ÿ ë¬¸ì¥: ['<start>', 'í•˜ë£¨', 'ê°€', 'ë˜', 'ê°€', 'ë„¤ìš”', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# <start>ì™€ <end> í† í°ì„ ans_corpusì— ì¶”ê°€\n",
    "ans_corpus = [[\"<start>\"] + tokens + [\"<end>\"] for tokens in ans_corpus]\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥\n",
    "print(\"ì²« ë²ˆì§¸ íƒ€ê²Ÿ ë¬¸ì¥:\", ans_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f362534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´ ì‚¬ì „ í¬ê¸°: 6834\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë“  ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    "all_sentences = que_corpus + ans_corpus\n",
    "\n",
    "# ë‹¨ì–´ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ ì‚¬ì „ ìƒì„±\n",
    "vocab = set([token for sentence in all_sentences for token in sentence])\n",
    "\n",
    "# ë‹¨ì–´ -> ì¸ë±ìŠ¤ ë§¤í•‘ ë° ë°˜ëŒ€ ë§¤í•‘ ìƒì„±\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}  # ì¸ë±ìŠ¤ëŠ” 1ë¶€í„° ì‹œì‘\n",
    "word2idx[\"<pad>\"] = 0  # íŒ¨ë”© í† í° ì¶”ê°€\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# ë‹¨ì–´ ì‚¬ì „ í¬ê¸° ì¶œë ¥\n",
    "print(f\"ë‹¨ì–´ ì‚¬ì „ í¬ê¸°: {len(word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5b1294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train shape: (11643, 20)\n",
      "dec_train shape: (7728, 20)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_corpus(corpus, word2idx, max_len=20):\n",
    "    \"\"\"\n",
    "    ë§ë­‰ì¹˜ë¥¼ ë²¡í„°í™”í•˜ëŠ” í•¨ìˆ˜.\n",
    "    Args:\n",
    "        corpus (list of list of str): í† í°í™”ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "        word2idx (dict): ë‹¨ì–´ -> ì¸ë±ìŠ¤ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬\n",
    "        max_len (int): ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ (ê¸°ë³¸ê°’: 20)\n",
    "    Returns:\n",
    "        np.ndarray: ë²¡í„°í™”ëœ ë¬¸ì¥ ë°°ì—´ (íŒ¨ë”© í¬í•¨)\n",
    "    \"\"\"\n",
    "    vectorized_data = []\n",
    "    for sentence in corpus:\n",
    "        vector = [word2idx.get(token, word2idx[\"<pad>\"]) for token in sentence]\n",
    "        # max_lenì— ë§ì¶° íŒ¨ë”© ì¶”ê°€ ë˜ëŠ” ìë¥´ê¸°\n",
    "        vector = vector[:max_len] + [word2idx[\"<pad>\"]] * (max_len - len(vector))\n",
    "        vectorized_data.append(vector)\n",
    "    return np.array(vectorized_data)\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„° ë²¡í„°í™”\n",
    "enc_train = vectorize_corpus(que_corpus, word2idx)\n",
    "dec_train = vectorize_corpus(ans_corpus, word2idx)\n",
    "\n",
    "# ë°ì´í„° í˜•íƒœ í™•ì¸\n",
    "print(f\"enc_train shape: {enc_train.shape}\")\n",
    "print(f\"dec_train shape: {dec_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab34b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ ì¸ì½”ë” ì…ë ¥: [1198 3349 2740 5278    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "ì²« ë²ˆì§¸ ë””ì½”ë” ì…ë ¥: [5963 3498 3659 4986 3659 2812  207 2638    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "ì²« ë²ˆì§¸ ì§ˆë¬¸ ë³µì›: ['12', 'ì‹œ', 'ë•¡', '!', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "ì²« ë²ˆì§¸ ë‹µë³€ ë³µì›: ['<start>', 'í•˜ë£¨', 'ê°€', 'ë˜', 'ê°€', 'ë„¤ìš”', '.', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(\"ì²« ë²ˆì§¸ ì¸ì½”ë” ì…ë ¥:\", enc_train[0])\n",
    "print(\"ì²« ë²ˆì§¸ ë””ì½”ë” ì…ë ¥:\", dec_train[0])\n",
    "\n",
    "# ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜í•´ë³´ê¸°\n",
    "print(\"ì²« ë²ˆì§¸ ì§ˆë¬¸ ë³µì›:\", [idx2word[idx] for idx in enc_train[0]])\n",
    "print(\"ì²« ë²ˆì§¸ ë‹µë³€ ë³µì›:\", [idx2word[idx] for idx in dec_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99fe9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6062431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "626accd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8e8949ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "45dca256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8cfbc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V ìˆœì„œì— ì£¼ì˜í•˜ì„¸ìš”!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5642c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7e32b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1ce549a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7ae6fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "462b5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "afa4142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cf9d00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae5d6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoderì˜ input\n",
    "    gold = tgt[:, 1:]     # Decoderì˜ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ right shiftë¥¼ í†µí•´ ìƒì„±í•œ ìµœì¢… íƒ€ê²Ÿ\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e8d45193",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_variant_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_195/1436080601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdataset_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtqdm_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{EPOCHS}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/cardinality.py\u001b[0m in \u001b[0;36mcardinality\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \"\"\"\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_variant_tensor'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(augmented_data).numpy()\n",
    "    tqdm_bar = tqdm(enumerate(augmented_data), total=dataset_count, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "\n",
    "    for batch, (src, tgt) in tqdm_bar:\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(src, tgt, transformer, optimizer)  \n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a93c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
